from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from typing import TypedDict, List, Optional, Dict, Any
import pandas as pd
import yaml
import os
import logging
import ast
import time

from src.utils.vector_store import VectorStoreRetriever

class AgentState(TypedDict):
    input: str
    intermediate_steps: List[str]
    final_classification: Optional[int]
    confidence_score: Optional[float]
    retrieved_examples: Optional[List[Dict[str, Any]]]
    label_distribution: Optional[Dict[int, float]]

class ChainOfThoughtPrompt:
    @staticmethod
    def generate_reasoning_prompt(
        text: str, 
        similar_examples: List[Dict[str, Any]], 
        label_distribution: Dict[int, float]
    ) -> str:
        """
        Generate a comprehensive reasoning prompt for GPT-4o with specific class descriptions
        
        Args:
            text (str): Input text to classify
            similar_examples (List[Dict]): Retrieved similar examples
            label_distribution (Dict): Distribution of labels in similar examples
        
        Returns:
            Detailed reasoning prompt
        """
        # Detailed class descriptions and examples
        class_descriptions = {
            0: {
                "name": "–£—á–µ–±–Ω–∞—è –∏ –≤–Ω–µ—É—á–µ–±–Ω–∞—è –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å",
                "description": """–û—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è–º –∏ –∫–∞—á–µ—Å—Ç–≤—É –ø—Ä–µ–ø–æ–¥–∞–≤–∞–Ω–∏—è –≤ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–µ, –≤–∫–ª—éÔøΩÔøΩ–∞—è:
                1. –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –æ—Ç–∑—ã–≤—ã –æ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–µ–ø–æ–¥–∞–≤–∞–Ω–∏—è –∏ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è—Ö
                2. –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞ (—Å–µ—Å—Å–∏—è, –∑–∞–¥–∞–Ω–∏—è, —ç–∫–∑–∞–º–µ–Ω—ã)
                3. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã –æ–±—É—á–µ–Ω–∏—è (–¥–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç, –ª–∏—á–Ω—ã–µ –∫–∞–±–∏–Ω–µ—Ç—ã)
                4. –í–Ω–µ—É—á–µ–±–Ω–∞—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–ø—Ä–æ—Ñ–∫–æ–º—ã, –∫—Ä—É–∂–∫–∏, –≤–æ–ª–æ–Ω—Ç–µ—Ä—Å—Ç–≤–æ)
                5. –°—Ç—É–¥–µ–Ω—á–µ—Å–∫–∏–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏""",
                "positive_examples": [
                    "–•–æ—á—É —Å–∫–∞–∑–∞—Ç—å –±–æ–ª—å—à–æ–µ —Å–ø–∞—Å–∏–±–æ –∫–∞—Ñ–µ–¥—Ä–µ –≤–∑—Ä–æ—Å–ª—ã—Ö –∏–Ω—Ñ–µ–∫—Ü–∏–æ–Ω–Ω—ã—Ö –±–æ–ª–µ–∑–Ω–µ–π! –î–æ—Ä–æ–≥–∏–µ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–∏, –í—ã —Ö–æ—Ä–æ—à–æ –æ–±—É—á–∞–µ—Ç–µ –≤–æ –≤—Ä–µ–º—è –≤—Å–µ–≥–æ —Ü–∏–∫–ª–∞, –æ—Ç–ª–∏—á–Ω—ã–µ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–∏!",
                    "–£ –ù–∞—Ç–∞–ª—å–∏ –ú–∏—Ö–∞–π–ª–æ–≤–Ω—ã –æ—á–µ–Ω—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –ª–µ–∫—Ü–∏–∏, —Å –∂–∏–≤–æ–π –ø–æ–¥–∞—á–µ–π –º–∞—Ç–µ—Ä–∏–∞–ª–∞...–ø—Ä–∏—Ö–æ–¥–∏—Ç–µ",
                    "¬´—è–∑—ã–∫–æ–≤–æ–π –∏–º–ø–µ—Ä–∏–∞–ª–∏–∑–º¬ª –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –ø–æ—Å—Ç–∞–≤–∏–ª –Ω–∞ –º–Ω–µ —Å–≤–æ—ë –∫–ª–µ–π–º–æ, –∞ –≤—Å—ë –ø–æ—Ç–æ–º—É, —á—Ç–æ —É –º–µ–Ω—è –Ω–µ –±—ã–ª–æ –ø—Ä–∞–≤–∞ –æ—Ç–∫–∞–∑–∞—Ç—å—Å—è –æ—Ç —ç—Ç–æ–≥–æ –ø—Ä–µ–¥–º–µ—Ç–∞"
                ],
                "negative_examples": [
                    "–í–ª–∞–¥–∏—Å–ª–∞–≤ –ò–≤–∞–Ω–æ–≤ –ª—É—á—à–∏–π –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å! –ì—Ä—É–ø–ø–∞ 5116 —Ç–∞–∫ —Å—á–∏—Ç–∞–µ—Ç! Ps: –ü–µ—Ç—Ä–æ–≤ –ù–∏–∫–∏—Ç–∞",
                    "‚ö°–ò–≤–∞–Ω–æ–≤–∞ –î–∞—à–∞‚ö° –°–ê–ú–´–ô –õ–£–ß–®–ò–ô –ü–†–û–§–û–†–ì –ù–ê –°–í–ï–¢–ï",
                    "–ö—Ç–æ-–Ω–∏–±—É–¥—å —Å–¥–∞–≤–∞–ª —ç–∫–∑–∞–º–µ–Ω –ø–æ –±—É—Ö–≥–∞–ª—Ç–µ—Ä—Å–∫–æ–º—É —É—á–µ—Ç—É —É –†–æ–º–∞–Ω–µ–Ω–∫–æ –û.–ï.? –°–ª–æ–∂–Ω–æ?"
                ]
            },
            1: {
                "name": "–°–æ—Ü–∏–∞–ª—å–Ω–æ-–±—ã—Ç–æ–≤—ã–µ —É—Å–ª–æ–≤–∏—è",
                "description": """–£—Å–ª–æ–≤–∏—è –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è –∏ –±—ã—Ç–∞ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤, –≤–∫–ª—é—á–∞—è:
                1. –ü—Ä–æ–±–ª–µ–º—ã –æ–±—â–µ–∂–∏—Ç–∏—è (—Ä–µ–º–æ–Ω—Ç, –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ, —É—Å–ª–æ–≤–∏—è)
                2. –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏ –∫–∞—á–µ—Å—Ç–≤–æ –∫–∞–º–ø—É—Å–∞
                3. –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è –ø–∏—Ç–∞–Ω–∏—è
                4. –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ
                5. –ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞""",
                "positive_examples": [
                    "–° 1 —Å–µ–Ω—Ç—è–±—Ä—è! –ö–æ–≥–¥–∞ –≤–∫–ª—é—á–∞—Ç –æ—Ç–æ–ø–ª–µÔøΩÔøΩ–∏–µ –≤ –æ–±—â–∞–≥–µ? –ú—ã —É–º–∏—Ä–∞–µ–º –æ—Ç —Ö–æ–ª–æ–¥–∞ ü•∂",
                    "–í –æ–±—â–µ–∂–∏—Ç–∏–∏ –æ–ø—è—Ç—å –ø—Ä–æ–±–ª–µ–º—ã —Å –≥–æ—Ä—è—á–µ–π –≤–æ–¥–æ–π, –∫–æ–≥–¥–∞ —ç—Ç–æ –∑–∞–∫–æ–Ω—á–∏—Ç—Å—è?",
                    "–ü–æ—á–µ–º—É –≤ —Å—Ç–æ–ª–æ–≤–æ–π —Ç–∞–∫–∏–µ –±–æ–ª—å—à–∏–µ –æ—á–µ—Ä–µ–¥–∏? –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ —É—Å–ø–µ—Ç—å –ø–æ–µ—Å—Ç—å –º–µ–∂–¥—É –ø–∞—Ä–∞–º–∏"
                ],
                "negative_examples": [
                    "–ö—Ç–æ —Ö–æ—á–µ—Ç –ø–æ–π—Ç–∏ –Ω–∞ –∫–æ–Ω—Ü–µ—Ä—Ç Schokk'a 17 —Ñ–µ–≤—Ä–∞–ª—è?",
                    "–ü—Ä–∏–Ω–∏–º–∞–µ—Ç –ª–∏ –°–µ–º–µ–Ω–æ–≤ –ê.–ú. –∞–≤—Ç–æ–º–∞—Ç—ã –æ—Ç –¥—Ä—É–≥–∏—Ö –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª–µ–π?",
                    "–í –∫–ª–∞—Å—Å –ú–∞—Ä–∏–∏ –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–Ω—ã —Ç—Ä–µ–±—É–µ—Ç—Å—è –∫–æ–Ω—Ü–µ—Ä—Ç–º–µ–π—Å—Ç–µ—Ä"
                ]
            },
            2: {
                "name": "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —É—Å–ª–æ–≤–∏—è",
                "description": """–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã –æ–±—É—á–µ–Ω–∏—è –∏ —Å—Ç—É–¥–µ–Ω—á–µ—Å–∫–æ–π –∂–∏–∑–Ω–∏:
                1. –°—Ç–∏–ø–µ–Ω–¥–∏–∏ –∏ —Å–æ—Ü–∏–∞–ª—å–Ω—ã–µ –≤—ã–ø–ª–∞—Ç—ã
                2. –ú–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–∞—è –ø–æ–º–æ—â—å
                3. –û–ø–ª–∞—Ç–∞ –æ–±—â–µ–∂–∏—Ç–∏—è
                4. –°—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
                5. –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ–¥—Ä–∞–±–æ—Ç–∫–∏""",
                "positive_examples": [
                    "–ü–æ—á–µ–º—É —Å–∏—Ä–æ—Ç–∞–º –ø—Ä–∏—à–ª–∞ —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç—å —Å—Ç–∏–ø–µ–Ω–¥–∏–∏, –∞ –∏–º–µ–Ω–Ω–æ —Ç–æ–ª—å–∫–æ —Å–æ—Ü–∏–∞–ª—å–Ω–∞—è. –ù–∞ —Å–∞–π—Ç–µ –¥—Ä—É–≥–∞—è —Å—É–º–º–∞.",
                    "–ö–æ–≥–¥–∞ –±—É–¥–µ—Ç –≤—ã–ø–ª–∞—Ç–∞ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–π –ø–æ–º–æ—â–∏? –£–∂–µ —Ç—Ä–µ—Ç–∏–π –º–µ—Å—è—Ü –∂–¥—É",
                    "–ü–æ–¥—Å–∫–∞–∂–∏—Ç–µ, –∫–∞–∫ –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —Å–æ—Ü–∏–∞–ª—å–Ω—É—é —Å—Ç–∏–ø–µ–Ω–¥–∏—é?"
                ],
                "negative_examples": [
                    "–ö—Ç–æ —Ö–æ—á–µ—Ç –ø–æ–π—Ç–∏ –Ω–∞ –∫–æ–Ω—Ü–µ—Ä—Ç –≤ —Å—É–±–±–æ—Ç—É?",
                    "–ì–¥–µ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é –Ω–µ–¥–µ–ª—é?",
                    "–ö—Ç–æ –∑–Ω–∞–µ—Ç —Ç–µ–ª–µ—Ñ–æ–Ω –¥–µ–∫–∞–Ω–∞—Ç–∞?"
                ]
            },
            3: {
                "name": "–õ–æ—è–ª—å–Ω–æ—Å—Ç—å –∫ –í–£–ó—É",
                "description": """–û–±—â–µ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—É –∏ –µ–≥–æ —Ä–µ–ø—É—Ç–∞—Ü–∏–∏:
                1. –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –ø—Ä–∏–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å –∫ –≤—É–∑—É
                2. –û—Ü–µ–Ω–∫–∞ –æ–±—â–µ–≥–æ —É—Ä–æ–≤–Ω—è —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞
                3. –û—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤—É –≤—É–∑–∞
                4. –ì–æ—Ä–¥–æ—Å—Ç—å –∑–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞
                5. –ö—Ä–∏—Ç–∏–∫–∞ –æ–±—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤—É–∑–∞
                6. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –≤—É–∑–∞–º–∏""",
                "positive_examples": [
                    "–ì–æ—Ä–∂—É—Å—å —Ä–æ–¥–Ω—ã–º –≤—É–∑–æ–º! ü´∂",
                    "–£ –º–µ–Ω—è —Å–ª—ë–∑—ã, –º—É—Ä–∞—à–∫–∏ –∏ –≥–æ—Ä–¥–æ—Å—Ç—å –∑–∞ —Å–∞–º—ã–π –ª—É—á—à–∏–π –í–£–ó‚ù§ –°–ø–∞—Å–∏–±–æ, –ò–º–ø–µ—Ä–∞—Ç–æ—Ä—Å–∫–∏–π!",
                    "–ö–∞–∫ –ø—Ä–∏—è—Ç–Ω–æ, —á—Ç–æ –Ω–∞—à–∞ –∞–∫–∞–¥–µ–º–∏—è —Ä–∞—Å—à–∏—Ä—è–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã —Å–≤–æ–µ–≥–æ –≤–ª–∏—è–Ω–∏—è"
                ],
                "negative_examples": [
                    "–æ—Ç—á–∏—Å–ª—è–π—Å—è, —É–Ω–∏–≤–µ—Ä - –¥–Ω–∏—â–µ –ø–æ–ª–Ω–æ–µ, —Ç–æ–ª—å–∫–æ –≤—ã–π–≥—Ä–∞–µ—à—å",
                    "–ù–∏—á–µ–≥–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –Ω–µ —Ä–µ—à–∞–µ—Ç—Å—è. –≠—Ç–æ—Ç –≤—É–∑ —É–∂–µ –Ω–µ —Å–ø–∞—Å—Ç–∏.",
                    "–ø–æ–ª —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –≤ —ç—Ç—É —à–∞—Ä–∞–≥—É –ø–æ—Å—Ç—É–ø–∞—é—Ç –∏ –Ω–µ –∑–Ω–∞—é—Ç –¥–ª—è —á–µ–≥–æ."
                ]
            }
        }

        # Prepare similar examples text
        examples_text = "\n".join([
            f"Example (Label: {ex['label']}, Similarity: {ex.get('distance', 'N/A')}):\n{ex['text']}"
            for ex in similar_examples
        ])
        
        # Prepare label distribution text
        label_dist_text = "\n".join([
            f"Label {label}: {prob * 100:.2f}%" 
            for label, prob in label_distribution.items()
        ])
        
        # Prepare class descriptions text
        class_desc_text = "\n\n".join([
            f"CLASS {label}: {desc['name']}\n"
            f"Description: {desc['description']}\n"
            f"Positive Examples: {desc['positive_examples']}\n"
            f"Negative Examples: {desc['negative_examples']}"
            for label, desc in class_descriptions.items()
        ])

        return f"""You are an expert text classifier specialized in student communication analysis. 
Your task is to perform a nuanced, multi-dimensional classification of student social media posts.

CLASSIFICATION CLASSES:
{class_desc_text}

CLASSIFICATION GUIDELINES:
1. Relevance Criteria:
   - Relevant posts contain:
     a) Student opinions, experiences, or feedback
     b) Discussions about university life
     c) Genuine concerns or observations
     d) Personal reflections on educational experiences

   - Irrelevant posts include:
     a) Advertisements
     b) Pure informational announcements
     c) Generic questions without context
     d) Spam or unrelated content
     e) Purely social interactions without educational context

INPUT TEXT:
{text}

CONTEXTUAL EVIDENCE:
- Similar Examples:
{examples_text}

- Label Distribution of Similar Examples:
{label_dist_text}

REASONING PROTOCOL:
1. Carefully analyze the input text against the class descriptions
2. Identify the most appropriate class based on thematic alignment
3. Provide a clear justification for your classification

RESPONSE FORMAT (Strict JSON):
{{
    "label": 0 or 1 or 2 or 3,  // Corresponding class label
    "confidence": 0.00-1.00,  // Confidence score
    "reasoning": "Comprehensive explanation of classification decision",
    "key_factors": [
        "Factor 1 description",
        "Factor 2 description"
    ]
}}

CRITICAL INSTRUCTIONS:
- Be precise and analytical
- Avoid ambiguity
- Provide clear, justifiable reasoning
- Consider the broader context of student communication
"""

class ClassificationAgent:
    def __init__(
        self, 
        feature_extractor, 
        classifier, 
        relevant_df: pd.DataFrame, 
        irrelevant_df: pd.DataFrame,
        config_path: str = 'config.yaml',
        validation_dataset: pd.DataFrame = None
    ):
        """
        Initialize classification agent with configuration
        
        Args:
            feature_extractor: Feature extraction utility
            classifier: Machine learning classifier
            relevant_df: DataFrame of relevant examples
            irrelevant_df: DataFrame of irrelevant examples
            config_path (str): Path to configuration file
            validation_dataset (pd.DataFrame, optional): Validation dataset
        """
        # Load configuration
        with open(config_path, 'r') as file:
            self.config = yaml.safe_load(file)
        
        # Use GPT-4o-mini with configuration settings
        self.llm = ChatOpenAI(
            model='gpt-4o-mini',  # Specify GPT-4o-mini
            temperature=self.config['system']['llm_temperature'],
            max_tokens=512,
            api_key=os.getenv('OPENAI_API_KEY')  # Ensure API key is set
        )
        
        self.feature_extractor = feature_extractor
        self.classifier = classifier
        
        # Initialize Vector Store
        self.vector_store = VectorStoreRetriever(
            config_path=config_path,
            validation_dataset=validation_dataset
        )
        self.vector_store.create_collection()
        
        # Populate Vector Store
        self._populate_vector_store(relevant_df, irrelevant_df)
    
    def _populate_vector_store(
        self, 
        relevant_df: pd.DataFrame, 
        irrelevant_df: pd.DataFrame
    ):
        """
        Populate vector store with training examples
        
        Args:
            relevant_df (pd.DataFrame): Relevant examples
            irrelevant_df (pd.DataFrame): Irrelevant examples
        """
        # If no dataframes provided, use validation dataset
        if relevant_df.empty and irrelevant_df.empty and self.vector_store.validation_dataset is not None:
            combined_df = self.vector_store.validation_dataset
        else:
            # Combine datasets
            combined_df = pd.concat([relevant_df, irrelevant_df])
        
        # Add documents to vector store
        self.vector_store.add_documents(
            documents=combined_df['post'].tolist(),
            labels=combined_df['label'].tolist() if 'label' in combined_df.columns else [0] * len(combined_df)
        )
    
    def initial_classification(self, state: AgentState, max_retries=3):
        """
        Perform classification using vector store retrieval and GPT-4o reasoning
        
        Args:
            state (AgentState): Current agent state
            max_retries (int): Number of retry attempts
        
        Returns:
            Updated agent state
        """
        for attempt in range(max_retries):
            try:
                # Existing classification logic remains the same
                text = state['input']
                
                # Retrieve Similar Examples
                retrieval_results = self.vector_store.query(text, n_results=5)
                
                # Get Label Distribution
                label_distribution = self.vector_store.get_label_distribution(retrieval_results)
                
                # Prepare Similar Examples
                similar_examples = [
                    {
                        'text': doc, 
                        'label': metadata['label'], 
                        'distance': distance
                    }
                    for doc, metadata, distance in zip(
                        retrieval_results['documents'], 
                        retrieval_results['metadatas'], 
                        retrieval_results['distances']
                    )
                ]
                
                # Generate Reasoning Prompt
                reasoning_prompt = ChainOfThoughtPrompt.generate_reasoning_prompt(
                    text, similar_examples, label_distribution
                )
                
                # LLM Reasoning with JSON output
                llm_response = self.llm.invoke(reasoning_prompt)
                
                # Robust parsing with multiple fallback methods
                try:
                    # First, try ast.literal_eval
                    classification_result = ast.literal_eval(llm_response.content)
                    
                    # Validate required keys
                    required_keys = ['label', 'confidence', 'reasoning']
                    for key in required_keys:
                        if key not in classification_result:
                            raise KeyError(f"Missing required key: {key}")
                    
                    # If parsing and validation succeed, return the result
                    return {
                        **state,
                        'intermediate_steps': [
                            f"Retrieved Examples: {similar_examples}",
                            f"Label Distribution: {label_distribution}",
                            f"LLM Reasoning: {classification_result.get('reasoning', '')}"
                        ],
                        'retrieved_examples': similar_examples,
                        'label_distribution': label_distribution,
                        'final_classification': classification_result['label'],
                        'confidence_score': classification_result['confidence']
                    }
                
                except (SyntaxError, ValueError, KeyError) as e:
                    logging.warning(f"Parsing attempt {attempt + 1} failed: {e}")
                    
                    # Optional: Add a slight delay between retries
                    if attempt < max_retries - 1:
                        time.sleep(0.5)
            
            except Exception as e:
                logging.error(f"Classification error on attempt {attempt + 1}: {e}")
        
        # If all retries fail, return default classification
        return {
            **state,
            'final_classification': self.config['classification']['default_label'],
            'confidence_score': 0.0,
            'error': "Failed to parse classification result after multiple attempts"
        }
    
    def create_graph(self):
        """
        Create LangGraph workflow for classification
        
        Returns:
            Compiled LangGraph workflow
        """
        workflow = StateGraph(AgentState)
        workflow.add_node("initial_classification", self.initial_classification)
        workflow.set_entry_point("initial_classification")
        workflow.add_edge("initial_classification", END)
        
        return workflow.compile() 